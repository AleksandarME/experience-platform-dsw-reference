{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Builder Actions Overview\n",
    "\n",
    "### Saving a File Cell\n",
    "If you wish to save the contents of a cell, simply run it.  The `%%writefile` command at the top of the cell will write the contents of the cell to the file named at the top of the cell. You should run the cells manually when applicable. However, **pressing any of the actions at the top will automatically run all file cells relevant to the action**.\n",
    "\n",
    "### Training and Scoring\n",
    "Press the associated buttons at the top in order to run training or scoring. The training output will be shown below the `evaluator.py` cell and scoring output will be shown below the `datasaver.py` cell. You must run training at least once before you can run scoring. You may delete the output cell(s). Running training the first time or after changing `requirements.txt` will be slower since the dependencies for the recipe need to be installed, but subsequent runs will be significantly faster. If you wish to see the hidden output add `debug` to the end of the output cell and re-run it.\n",
    "\n",
    "### Creating the Recipe\n",
    "When you are done editing the recipe and satisfied with the training/scoring output, you can create a recipe from the notebook by pressing `Create Recipe`. You must run scoring at least once before you can create the recipe. After pressing it, you will see a progress bar showing how much time is left for the build to finish. If the recipe creation is successful the progress bar will be replaced by an external link that you can click to navigate to the created recipe.\n",
    "\n",
    "\n",
    "## Caution!\n",
    "* **Do not delete any of the file cells**\n",
    "* **Do not edit the `%%writefile` line at the top of the file cells**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Requirements File** (Optional)\n",
    "Add additional libraries you wish to use in the recipe to the cell below. You can specify the version number if necessary. The file cell below is a **commented out example**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "requirements.txt",
    "tags": [
     "requirements.txt"
    ]
   },
   "outputs": [],
   "source": [
    "# pandas=0.22.0\n",
    "# numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search here for additional libraries https://anaconda.org/. This is the list of main **libraries already in use**:\n",
    "`python=3.5.2` `scikit-learn` `pandas` `numpy` `data_access_sdk_python`\n",
    "**Warning: libraries or specific versions you add may be incompatible with the above libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuration Files**\n",
    "List any hyperparameters you wish to use. Specify the dataset(s) and schema(s) that are needed for training/scoring. To find the dataset ids go to the **Data tab** in Adobe Experience Platform or view the **Datasets** folder in the **Notebooks Data tab** on the left. You can also find schema id in the **Notebooks Data tab** under the **Schemas** folder. Each configuration will only be used for its corresponding action. `ACP_DSW_TRAINING_XDM_SCHEMA` and `ACP_DSW_SCORING_RESULTS_XDM_SCHEMA` will only be used after the recipe has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training.conf",
    "tags": [
     "training.conf"
    ]
   },
   "outputs": [],
   "source": [
    "{\n",
    "   \"tenant_id\": \"<replace with tenant id>\",\n",
    "   \"trainingDataSetId\": \"<replace with training dataset id>\",\n",
    "   \"ACP_DSW_TRAINING_XDM_SCHEMA\": \"<replace with training xdm schema id>\",\n",
    "   \"num_recommendations\": \"5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoring Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scoring.conf",
    "tags": [
     "scoring.conf"
    ]
   },
   "outputs": [],
   "source": [
    "{\n",
    "   \"tenant_id\": \"<replace with tenant id>\",\n",
    "   \"scoringDataSetId\": \"<replace with scoring input dataset id>\",\n",
    "   \"scoringResultsDataSetId\": \"<replace with scoring results dataset id>\",\n",
    "   \"ACP_DSW_SCORING_RESULTS_XDM_SCHEMA\": \"<replace with scoring results xdm schema id>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following configuration parameters are automatically set for you when you train/score:** \n",
    "`ML_FRAMEWORK_IMS_USER_CLIENT_ID` `ML_FRAMEWORK_IMS_TOKEN` `ML_FRAMEWORK_IMS_ML_TOKEN` `ML_FRAMEWORK_IMS_TENANT_ID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Data Loader File**\n",
    "Implement the `load` function to load and prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainingdataloader.py",
    "tags": [
     "trainingdataloader.py"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_access_sdk_python.reader import DataSetReader\n",
    "\n",
    "def load(configProperties):\n",
    "    print(\"Training Data Load Start\")\n",
    "    prodreader = DataSetReader(client_id=configProperties['ML_FRAMEWORK_IMS_USER_CLIENT_ID'],\n",
    "                               user_token=configProperties['ML_FRAMEWORK_IMS_TOKEN'],\n",
    "                               service_token=configProperties['ML_FRAMEWORK_IMS_ML_TOKEN'])\n",
    "\n",
    "    train_data = prodreader.load(data_set_id=configProperties['trainingDataSetId'],\n",
    "                         ims_org=configProperties['ML_FRAMEWORK_IMS_TENANT_ID'])\n",
    "\n",
    "    print(\"Training Data Load Finish\")\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scoring Data Loader File**\n",
    "Implement the `load` function to load and prepare the scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scoringdataloader.py",
    "tags": [
     "scoringdataloader.py"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_access_sdk_python.reader import DataSetReader\n",
    "\n",
    "def load(configProperties):\n",
    "\n",
    "    print(\"Scoring Data Load Start\")\n",
    "\n",
    "    prodreader = DataSetReader(client_id=configProperties['ML_FRAMEWORK_IMS_USER_CLIENT_ID'],\n",
    "                               user_token=configProperties['ML_FRAMEWORK_IMS_TOKEN'],\n",
    "                               service_token=configProperties['ML_FRAMEWORK_IMS_ML_TOKEN'])\n",
    "\n",
    "    df = prodreader.load(data_set_id=configProperties['scoringDataSetId'],\n",
    "                         ims_org=configProperties['ML_FRAMEWORK_IMS_TENANT_ID'])\n",
    "\n",
    "    print(\"Scoring Data Load Finish\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline File**\n",
    "Implement the `train` function and return the trained model. Implement the `score` function to return a prediction made on the scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline.py",
    "tags": [
     "pipeline.py"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "class CooccurrenceBasedRecommendationModel():\n",
    "    def __init__(self, configProperties):\n",
    "        self.num_to_recommend = int(configProperties['num_recommendations'])\n",
    "        self.recommendations = ['dummy']\n",
    "        tenant_id = configProperties['tenant_id']\n",
    "        self.user_id_column = '_%s.userId' % tenant_id\n",
    "        self.recommendations_column = '_%s.recommendations' % tenant_id\n",
    "        self.item_id_column = '_%s.itemId' % tenant_id\n",
    "        self.interactions_column_name = 'interactions'\n",
    "        self.cooc_matrix = {}\n",
    "\n",
    "    def fit(self, df):\n",
    "        df = df[df[self.item_id_column].notnull()]\n",
    "        self.recommendations = [item for item, freq in\n",
    "                                Counter(list(df[self.item_id_column].values)).most_common(self.num_to_recommend)]\n",
    "        # I want to first group items by user id so that we can calculate co-occurrence\n",
    "        print('grouping data by user id')\n",
    "        data_grouped_by_user = df.groupby(self.user_id_column).agg(\n",
    "            {self.item_id_column: lambda x: '#'.join(x)})\\\n",
    "            .rename(columns={self.item_id_column:self.interactions_column_name}).reset_index()\n",
    "\n",
    "        # now calculate co-occurrence\n",
    "        print('building the cooccurrence matrix')\n",
    "        for index, row in data_grouped_by_user.iterrows():\n",
    "            list_interactions = row[self.interactions_column_name].split('#')\n",
    "            # get all pairwise combinations\n",
    "            for (item_1, item_2) in list(itertools.combinations(list_interactions, 2)):\n",
    "                self.update_coocur_matrix(item_1, item_2)\n",
    "                self.update_coocur_matrix(item_2, item_1)\n",
    "\n",
    "        print('fit complete')\n",
    "\n",
    "    def update_coocur_matrix(self, item_1, item_2):\n",
    "        if item_1 in self.cooc_matrix:\n",
    "            # entry already exists for this item\n",
    "            if item_2 in self.cooc_matrix[item_1]:\n",
    "                # entry exists for this item pair\n",
    "                self.cooc_matrix[item_1][item_2] += 1\n",
    "            else:\n",
    "                # create a new entry for this item pair\n",
    "                self.cooc_matrix[item_1][item_2] = 1\n",
    "        else:\n",
    "            # create a new entry for this item\n",
    "            self.cooc_matrix[item_1] = {}\n",
    "            self.cooc_matrix[item_1][item_2] = 1\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        # remove columns having none\n",
    "        df = df[df[self.item_id_column].notnull()]\n",
    "\n",
    "        df_grouped_by_user = df.groupby(self.user_id_column).agg(\n",
    "            {self.item_id_column: lambda x: '#'.join(x)})\\\n",
    "        .rename(columns={self.item_id_column:self.interactions_column_name}).reset_index()\n",
    "\n",
    "        # now for each user, we want to get recommendations based on the last seen item\n",
    "        def recommend_items(row):\n",
    "            list_interactions = row[self.interactions_column_name].split('#')\n",
    "            last_interaction = list_interactions[-1]\n",
    "            # lookup last interaction in the cooccurrence matrix\n",
    "            if last_interaction in self.cooc_matrix:\n",
    "                # sort cooccurrence matrix in descending order\n",
    "                items_cooccurred = self.cooc_matrix[last_interaction]\n",
    "                recommendations = []\n",
    "                for item, cooc_count in sorted(items_cooccurred.items(), key=operator.itemgetter(0), reverse=True):\n",
    "                    recommendations.append(item)\n",
    "                    if len(recommendations) == self.num_to_recommend:\n",
    "                        break\n",
    "                # return personalized recommendations\n",
    "                return '#'.join(recommendations)\n",
    "            else:\n",
    "                # give default recommendations\n",
    "                return '#'.join(self.recommendations)\n",
    "\n",
    "\n",
    "        df_grouped_by_user[self.recommendations_column] = df_grouped_by_user.apply(lambda row: recommend_items(row), axis=1)\n",
    "        df_grouped_by_user = df_grouped_by_user.drop([self.interactions_column_name],axis=1)\n",
    "\n",
    "        return df_grouped_by_user\n",
    "\n",
    "def train(configProperties, data):\n",
    "\n",
    "    print(\"Train Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Fit model\n",
    "    #########################################\n",
    "    model = CooccurrenceBasedRecommendationModel(configProperties)\n",
    "\n",
    "    model.fit(data)\n",
    "\n",
    "    print(\"Train Complete\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def score(configProperties, data, model):\n",
    "\n",
    "    print(\"Score Start\")\n",
    "\n",
    "    result = model.predict(data)\n",
    "\n",
    "    print(\"Score Complete\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluator File**\n",
    "Implement the `split` function to partition the training data and the `evaluate` function to the return the validation metrics you wish to see. Training output will be shown below this file cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluator.py",
    "tags": [
     "evaluator.py"
    ]
   },
   "outputs": [],
   "source": [
    "from ml.runtime.python.Interfaces.AbstractEvaluator import AbstractEvaluator\n",
    "from data_access_sdk_python.reader import DataSetReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Evaluator(AbstractEvaluator):\n",
    "\n",
    "    def split(self, configProperties={}, dataframe=None):\n",
    "        #########################################\n",
    "        # Load Data\n",
    "        #########################################\n",
    "        train = dataframe[:]\n",
    "        test = dataframe[:]\n",
    "\n",
    "        return train, test\n",
    "\n",
    "    def evaluate(self, data=[], model={}, configProperties={}):\n",
    "        print (\"Evaluation evaluate triggered\")\n",
    "\n",
    "        # remove columns having none\n",
    "        data = data[data[model.item_id_column].notnull()]\n",
    "\n",
    "        data_grouped_by_user = data.groupby(model.user_id_column).agg(\n",
    "            {model.item_id_column: lambda x: '#'.join(x)})\\\n",
    "        .rename(columns={model.item_id_column: model.interactions_column_name}).reset_index()\n",
    "\n",
    "        data_recommendations = model.predict(data)\n",
    "\n",
    "        merged_df = pd.merge(data_grouped_by_user, data_recommendations, on=[model.user_id_column]).reset_index()\n",
    "\n",
    "        def compute_recall(row):\n",
    "            set_interactions = set(row[model.interactions_column_name].split('#'))\n",
    "            set_recommendations = set(row[model.recommendations_column].split('#'))\n",
    "            inters = set_interactions.intersection(set_recommendations)\n",
    "            if len(inters) > 0:\n",
    "                return 1\n",
    "            return 0\n",
    "\n",
    "        def compute_precision(row):\n",
    "            set_interactions = set(row[model.interactions_column_name].split('#'))\n",
    "            list_recommendations = row[model.recommendations_column].split('#')\n",
    "            score = 0\n",
    "            weight = 0.5\n",
    "            for rec in list_recommendations:\n",
    "                if rec in set_interactions:\n",
    "                    score = score + weight\n",
    "                weight = weight / 2\n",
    "\n",
    "            return score\n",
    "\n",
    "\n",
    "        merged_df['recall'] = merged_df.apply(lambda row: compute_recall(row), axis=1)\n",
    "        merged_df['precision'] = merged_df.apply(lambda row: compute_precision(row), axis=1)\n",
    "\n",
    "        recall = merged_df['recall'].mean()\n",
    "        precision = merged_df['precision'].mean()\n",
    "\n",
    "        metric = [{\"name\": \"Recall\", \"value\": recall, \"valueType\": \"double\"},\n",
    "                 {\"name\": \"Precision\", \"value\": precision, \"valueType\": \"double\"}]\n",
    "\n",
    "        print(metric)\n",
    "\n",
    "        return metric\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Saver File**\n",
    "Implement the `save` function for saving your prediction. Scoring output will be added below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datasaver.py",
    "tags": [
     "datasaver.py"
    ]
   },
   "outputs": [],
   "source": [
    "from data_access_sdk_python.writer import DataSetWriter\n",
    "\n",
    "def save(configProperties, prediction):\n",
    "    print(prediction)\n",
    "    prodwriter = DataSetWriter(client_id=configProperties['ML_FRAMEWORK_IMS_USER_CLIENT_ID'],\n",
    "                               user_token=configProperties['ML_FRAMEWORK_IMS_TOKEN'],\n",
    "                               service_token=configProperties['ML_FRAMEWORK_IMS_ML_TOKEN'])\n",
    "\n",
    "    batch_id = prodwriter.write(data_set_id=configProperties['scoringResultsDataSetId'],\n",
    "                 dataframe=prediction,\n",
    "                 ims_org=configProperties['ML_FRAMEWORK_IMS_TENANT_ID'])\n",
    "    print(\"Data written successfully to platform:\",batch_id)\n"
   ]
  }
 ],
 "metadata": {
  "elementId": "OSakdN-Q1M",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notebook_type": "builder"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
