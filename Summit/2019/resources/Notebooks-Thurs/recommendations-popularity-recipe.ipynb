{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Builder Actions Overview\n",
    "\n",
    "### Saving a File Cell\n",
    "If you wish to save the contents of a cell, simply run it.  The `%%writefile` command at the top of the cell will write the contents of the cell to the file named at the top of the cell. You should run the cells manually when applicable. However, **pressing any of the actions at the top will automatically run all file cells relevant to the action**.\n",
    "\n",
    "### Training and Scoring\n",
    "Press the associated buttons at the top in order to run training or scoring. The training output will be shown below the `pipeline.py` cell and scoring output will be shown below the `datasaver.py` cell. You must run training at least once before you can run scoring. You may delete the output cell(s). Running training the first time or after changing `requirements.txt` will be slower since the dependencies for the recipe need to be installed, but subsequent runs will be signigicantly faster.\n",
    "\n",
    "### Creating the Recipe\n",
    "When you are done editing the recipe and satisfied with the training/scoring output, you can create a recipe from the notebook by pressing `Create Recipe`. After pressing it, you will see a spinner that will spin until the recipe creation has finished. If the recipe creation is successful the spinner will be replaced by an external link that you can click to navigate to the created recipe.\n",
    "\n",
    "\n",
    "## Caution!\n",
    "* **Do not delete any of the file cells**\n",
    "* **Do not edit the `%%writefile` line at the top of the file cells**\n",
    "* **Do not refresh the JupyterLab page while the recipe is being created**\n",
    "</br>\n",
    "</br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Requirements file** (Optional)\n",
    "Add additional libraries you wish to use in the recipe to the cell below. You can specify the version number if necessary. The file cell below is a **commented out example**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "requirements.txt",
    "tags": [
     "requirements.txt"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# pandas=0.22.0\n",
    "# numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search here for additional libraries https://anaconda.org/. This is the list of main **libraries already in use**: </br>\n",
    "`python=3.5.2` `scikit-learn` `pandas` `numpy` `data_access_sdk_python` </br>\n",
    "**Warning: libraries or specific versions you add may be incompatible with the above libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuration file**\n",
    "Specify the dataset(s) you wish to use for training/scoring and add hyperparameters. To find the dataset ids go to the **Data tab** in Adobe Experience Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "recipe.conf",
    "tags": [
     "recipe.conf"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "{\n",
    "    \"trainingDataSetId\": \"5c8c31db3603991515a6e2da\",\n",
    "    \"scoringDataSetId\": \"5c8c31db3603991515a6e2da\",\n",
    "    \"scoringResultsDataSetId\":\"5c913bbf16f0dc151609b69d\",\n",
    "    \"ACP_DSW_TRAINING_XDM_SCHEMA\":\"https://ns.adobe.com/platformlab/schemas/764807fb1f2503047c09945e46cfdf30\",\n",
    "    \"ACP_DSW_SCORING_RESULTS_XDM_SCHEMA\":\"https://ns.adobe.com/platformlab/schemas/637f4724918fe241b2e6d1cdf8353c39\",\n",
    "    \"num_recommendations\": \"5\",\n",
    "    \"sampling_fraction\": \"0.5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following configuration parameters are automatically set for you when you train/score:** </br>\n",
    "`ML_FRAMEWORK_IMS_USER_CLIENT_ID` `ML_FRAMEWORK_IMS_TOKEN` `ML_FRAMEWORK_IMS_ML_TOKEN` `ML_FRAMEWORK_IMS_TENANT_ID` `saveData`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluator file**\n",
    "Fill in how you wish to evaluate your trained recipe and how your training data should be split. You can also use this file to load and prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluator.py",
    "tags": [
     "evaluator.py"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from ml.runtime.python.Interfaces.AbstractEvaluator import AbstractEvaluator\n",
    "from data_access_sdk_python.reader import DataSetReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Evaluator(AbstractEvaluator):\n",
    "    def __init__(self):\n",
    "        print(\"Initiate\")\n",
    "        self.user_id_column = '_platformlab.userId'\n",
    "        self.recommendations_column = '_platformlab.recommendations'\n",
    "        self.item_id_column = '_platformlab.itemId'\n",
    "\n",
    "\n",
    "    def evaluate(self, data=[], model={}, configProperties={}):\n",
    "        print (\"Evaluation evaluate triggered\")\n",
    "        \n",
    "        # remove columns having none\n",
    "        data = data[data[self.item_id_column].notnull()]\n",
    "        \n",
    "        data_grouped_by_user = data.groupby(self.user_id_column).agg(\n",
    "            {self.item_id_column: lambda x: '#'.join(x)})\\\n",
    "        .rename(columns={self.item_id_column:'interactions'}).reset_index()\n",
    "        \n",
    "        data_recommendations = model.predict(data)\n",
    "        \n",
    "        merged_df = pd.merge(data_grouped_by_user, data_recommendations, on=[self.user_id_column]).reset_index()\n",
    "        \n",
    "        def compute_recall(row):\n",
    "            set_interactions = set(row['interactions'].split('#'))\n",
    "            set_recommendations = set(row[self.recommendations_column].split('#'))\n",
    "            inters = set_interactions.intersection(set_recommendations)\n",
    "            if len(inters) > 0:\n",
    "                return 1\n",
    "            return 0\n",
    "        \n",
    "        def compute_precision(row):\n",
    "           set_interactions = set(row['interactions'].split('#'))\n",
    "           list_recommendations = row[self.recommendations_column].split('#')\n",
    "           score = 0\n",
    "           weight = 0.5\n",
    "           for rec in list_recommendations:\n",
    "               if rec in set_interactions:\n",
    "                   score = score + weight\n",
    "               weight = weight / 2\n",
    "\n",
    "           return score\n",
    "\n",
    "\n",
    "        merged_df['recall'] = merged_df.apply(lambda row: compute_recall(row), axis=1)\n",
    "        merged_df['precision'] = merged_df.apply(lambda row: compute_precision(row), axis=1)\n",
    "\n",
    "        recall = merged_df['recall'].mean()\n",
    "        precision = merged_df['precision'].mean()\n",
    "\n",
    "        metric = [{\"name\": \"Recall\", \"value\": recall, \"valueType\": \"double\"},\n",
    "                 {\"name\": \"Precision\", \"value\": precision, \"valueType\": \"double\"}]\n",
    "\n",
    "        print(metric)\n",
    "\n",
    "        return metric\n",
    "\n",
    "    def split(self, configProperties={}):\n",
    "        #########################################\n",
    "        # Load Data\n",
    "        #########################################\n",
    "        prodreader = DataSetReader(client_id=configProperties['ML_FRAMEWORK_IMS_USER_CLIENT_ID'],\n",
    "                                   user_token=configProperties['ML_FRAMEWORK_IMS_TOKEN'],\n",
    "                                   service_token=configProperties['ML_FRAMEWORK_IMS_ML_TOKEN'])\n",
    "\n",
    "        df = prodreader.load(data_set_id=configProperties['trainingDataSetId'],\n",
    "                             ims_org=configProperties['ML_FRAMEWORK_IMS_TENANT_ID'])\n",
    "\n",
    "        train = df[:]\n",
    "        test = df[:]\n",
    "\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Data Loader file**\n",
    "Call your Evaluator split here and/or use this file to load and prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainingdataloader.py",
    "tags": [
     "trainingdataloader.py"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_access_sdk_python.reader import DataSetReader\n",
    "\n",
    "from recipe.evaluator import Evaluator\n",
    "\n",
    "def load(configProperties):\n",
    "    print(\"Training Data Load Start\")\n",
    "    evaluator = Evaluator()\n",
    "    (train_data, _) = evaluator.split(configProperties)\n",
    "\n",
    "    print(\"Training Data Load Finish\")\n",
    "    return train_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scoring Data Loader file**\n",
    "Use this file to load and prepare your scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scoringdataloader.py",
    "tags": [
     "scoringdataloader.py"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_access_sdk_python.reader import DataSetReader\n",
    "\n",
    "def load(configProperties):\n",
    "\n",
    "    print(\"Scoring Data Load Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Load Data\n",
    "    #########################################\n",
    "    prodreader = DataSetReader(client_id=configProperties['ML_FRAMEWORK_IMS_USER_CLIENT_ID'],\n",
    "                               user_token=configProperties['ML_FRAMEWORK_IMS_TOKEN'],\n",
    "                               service_token=configProperties['ML_FRAMEWORK_IMS_ML_TOKEN'])\n",
    "\n",
    "    df = prodreader.load(data_set_id=configProperties['scoringDataSetId'],\n",
    "                         ims_org=configProperties['ML_FRAMEWORK_IMS_TENANT_ID'])\n",
    "\n",
    "    print(\"Scoring Data Load Finish\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline file**\n",
    "Fill in the training and scoring functions for your recipe. Training output will be added below this file cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline.py",
    "tags": [
     "pipeline.py"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class PopularityBasedRecommendationModel():\n",
    "    def __init__(self, num_to_recommend):\n",
    "        self.num_to_recommend = num_to_recommend\n",
    "        self.recommendations = ['dummy']\n",
    "        self.user_id_column = '_platformlab.userId'\n",
    "        self.recommendations_column = '_platformlab.recommendations'\n",
    "        self.item_id_column = '_platformlab.itemId'\n",
    "    \n",
    "    def fit(self, df):\n",
    "        df = df[df[self.item_id_column].notnull()]\n",
    "        self.recommendations = [item for item, freq in \n",
    "                                Counter(list(df[self.item_id_column].values)).most_common(self.num_to_recommend)]\n",
    "\n",
    "        \n",
    "    def predict(self, df):\n",
    "        # remove columns having none\n",
    "        df = df[df[self.item_id_column].notnull()]\n",
    "        \n",
    "        df_grouped_by_user = df.groupby(self.user_id_column).agg(\n",
    "            {self.item_id_column: lambda x: ','.join(x)})\\\n",
    "        .rename(columns={self.item_id_column:'interactions'}).reset_index()\n",
    "        \n",
    "        df_grouped_by_user[self.recommendations_column] = '#'.join(self.recommendations)\n",
    "        df_grouped_by_user = df_grouped_by_user.drop(['interactions'],axis=1)\n",
    "        \n",
    "        return df_grouped_by_user\n",
    "\n",
    "def train(configProperties, data):\n",
    "\n",
    "    print(\"Train Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Extract fields from configProperties\n",
    "    #########################################\n",
    "    num_recommendations = int(configProperties['num_recommendations'])\n",
    "\n",
    "    #########################################\n",
    "    # Fit model\n",
    "    #########################################\n",
    "    model = PopularityBasedRecommendationModel(num_recommendations)\n",
    "\n",
    "    model.fit(data)\n",
    "\n",
    "    print(\"Train Complete\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def score(configProperties, data, model):\n",
    "\n",
    "    print(\"Score Start\")\n",
    "\n",
    "    result = model.predict(data)\n",
    "\n",
    "    print(\"Score Complete\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Saver file**\n",
    "Add how you wish to save your scored data. **By default saveData=False, since saving data is not relevant when scoring from the notebook.** Scoring output will be added below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datasaver.py",
    "tags": [
     "datasaver.py"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "from data_access_sdk_python.writer import DataSetWriter\n",
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "def save(configProperties, prediction):\n",
    "    \n",
    "    print(prediction)\n",
    "    prodwriter = DataSetWriter(client_id=configProperties['ML_FRAMEWORK_IMS_USER_CLIENT_ID'],\n",
    "                               user_token=configProperties['ML_FRAMEWORK_IMS_TOKEN'],\n",
    "                               service_token=configProperties['ML_FRAMEWORK_IMS_ML_TOKEN'])\n",
    "    \n",
    "    batch_id = prodwriter.write(data_set_id=configProperties['scoringResultsDataSetId'],\n",
    "                 dataframe=prediction,\n",
    "                 ims_org=configProperties['ML_FRAMEWORK_IMS_TENANT_ID'])\n",
    "    print(\"Data written successfully to platform:\",batch_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notebook_type": "builder"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
